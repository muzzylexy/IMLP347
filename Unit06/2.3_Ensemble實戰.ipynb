{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3129ab3d",
   "metadata": {},
   "source": [
    "# Ensemble/Voting Classification in Python with Scikit-Learn\n",
    "refï¼šhttps://www.kaggle.com/c/titanic/submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6b020b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import result\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7163b994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "Testing Data:\n",
      "PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.read_csv(\"data/train.csv\")\n",
    "testing_data = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "def get_nulls(training, testing):\n",
    "    print(\"Training Data:\")\n",
    "    print(pd.isnull(training).sum())\n",
    "    print(\"Testing Data:\")\n",
    "    print(pd.isnull(testing).sum())\n",
    "\n",
    "get_nulls(training_data, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffd6109f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "Survived    0\n",
      "Pclass      0\n",
      "Sex         0\n",
      "Age         0\n",
      "SibSp       0\n",
      "Parch       0\n",
      "Fare        0\n",
      "Embarked    0\n",
      "dtype: int64\n",
      "Testing Data:\n",
      "Pclass      0\n",
      "Sex         0\n",
      "Age         0\n",
      "SibSp       0\n",
      "Parch       0\n",
      "Fare        0\n",
      "Embarked    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop the cabin column, as there are too many missing values\n",
    "# Drop the ticket numbers too, as there are too many categories\n",
    "# Drop names as they won't really help predict survivors\n",
    "testing_passenger_ids = testing_data[\"PassengerId\"].values\n",
    "\n",
    "training_data.drop(\n",
    "    [\"Name\", \"Ticket\", \"Cabin\", \"PassengerId\"], axis=1, inplace=True)\n",
    "testing_data.drop([\"Name\", \"Ticket\", \"Cabin\", \"PassengerId\"],\n",
    "                  axis=1, inplace=True)\n",
    "\n",
    "# Taking the mean/average value would be impacted by the skew\n",
    "# so we should use the median value to impute missing values\n",
    "training_data[\"Age\"].fillna(training_data[\"Age\"].median(), inplace=True)\n",
    "\n",
    "training_data[\"Embarked\"].fillna(\n",
    "    training_data[\"Embarked\"].mode()[0], inplace=True)\n",
    "testing_data[\"Age\"].fillna(testing_data[\"Age\"].median(), inplace=True)\n",
    "testing_data[\"Fare\"].fillna(testing_data[\"Fare\"].median(), inplace=True)\n",
    "\n",
    "get_nulls(training_data, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19e4f59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare Embarked\n",
      "0         0       3    1  22.0      1      0   7.2500        S\n",
      "1         1       1    0  38.0      1      0  71.2833        C\n",
      "2         1       3    0  26.0      0      0   7.9250        S\n",
      "3         1       1    0  35.0      1      0  53.1000        S\n",
      "4         0       3    1  35.0      0      0   8.0500        S\n"
     ]
    }
   ],
   "source": [
    "# Fit the encoder on the data (Feature: Sex)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(training_data[\"Sex\"])\n",
    "\n",
    "# Transform and replace training data\n",
    "training_data[\"Sex\"] = encoder.transform(training_data[\"Sex\"])\n",
    "testing_data[\"Sex\"] = encoder.transform(testing_data[\"Sex\"])\n",
    "\n",
    "print(training_data.head())\n",
    "# Fit the encoder on the data (Feature: Embarked)\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "embarked_data_arr = np.array(training_data[\"Embarked\"]).reshape(-1, 1)\n",
    "\n",
    "# Encode training data\n",
    "encoder.fit(embarked_data_arr)\n",
    "encoded_array = encoder.transform(embarked_data_arr).toarray()\n",
    "training_data.drop([\"Embarked\"], axis=1, inplace=True)\n",
    "training_data = pd.concat([training_data, pd.DataFrame(\n",
    "    data=encoded_array, columns=[\"Embarked_1\", \"Embarked_2\", \"Embarked_3\"])], axis=1)\n",
    "\n",
    "# Encode testing data\n",
    "embarked_data_arr = np.array(testing_data[\"Embarked\"]).reshape(-1, 1)\n",
    "encoded_array = encoder.transform(embarked_data_arr).toarray()\n",
    "testing_data.drop([\"Embarked\"], axis=1, inplace=True)\n",
    "testing_data = pd.concat([testing_data, pd.DataFrame(\n",
    "    data=encoded_array, columns=[\"Embarked_1\", \"Embarked_2\", \"Embarked_3\"])], axis=1)\n",
    "\n",
    "# Scaling the data (Feature: Age, Fare)\n",
    "age_scalar = StandardScaler()\n",
    "age_scalar.fit(training_data[\"Age\"].values.reshape(-1, 1))\n",
    "\n",
    "training_data[\"Age\"] = age_scalar.transform(\n",
    "    training_data[\"Age\"].values.reshape(-1, 1))\n",
    "\n",
    "testing_data[\"Age\"] = age_scalar.transform(\n",
    "    testing_data[\"Age\"].values.reshape(-1, 1))\n",
    "\n",
    "fare_scalar = StandardScaler()\n",
    "fare_scalar.fit(training_data[\"Fare\"].values.reshape(-1, 1))\n",
    "\n",
    "training_data[\"Fare\"] = fare_scalar.transform(\n",
    "    training_data[\"Fare\"].values.reshape(-1, 1))\n",
    "testing_data[\"Fare\"] = fare_scalar.transform(\n",
    "    testing_data[\"Fare\"].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65bd0090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features = \n",
      "    Pclass  Sex       Age  SibSp  Parch      Fare  Embarked_1  Embarked_2  \\\n",
      "0       3    1 -0.565736      1      0 -0.502445         0.0         0.0   \n",
      "1       1    0  0.663861      1      0  0.786845         1.0         0.0   \n",
      "2       3    0 -0.258337      0      0 -0.488854         0.0         0.0   \n",
      "3       1    0  0.433312      1      0  0.420730         0.0         0.0   \n",
      "4       3    1  0.433312      0      0 -0.486337         0.0         0.0   \n",
      "\n",
      "   Embarked_3  \n",
      "0         1.0  \n",
      "1         0.0  \n",
      "2         1.0  \n",
      "3         1.0  \n",
      "4         1.0  \n",
      "Labels = \n",
      " 0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: Survived, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Now to select our training/testing data\n",
    "features = training_data.drop([\"Survived\"], axis=1)\n",
    "labels = training_data[\"Survived\"]\n",
    "\n",
    "print(\"Features = \\n\", features.head())\n",
    "print(\"Labels = \\n\", labels.head())\n",
    "\n",
    "# Make the train/test data from validation\n",
    "x_train, x_eval, y_train, y_eval = train_test_split(\n",
    "    features, labels, test_size=0.1, random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5652bc34",
   "metadata": {},
   "source": [
    "## Simple Averaging Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd452cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "LogReg_clf = LogisticRegression()\n",
    "DTree_clf = DecisionTreeClassifier()\n",
    "SVC_clf = SVC(probability=True)\n",
    "\n",
    "LogReg_clf.fit(x_train, y_train)\n",
    "DTree_clf.fit(x_train, y_train)\n",
    "SVC_clf.fit(x_train, y_train)\n",
    "\n",
    "LogReg_pred = LogReg_clf.predict(x_eval)\n",
    "DTree_pred = DTree_clf.predict(x_eval)\n",
    "SVC_pred = SVC_clf.predict(x_eval)\n",
    "\n",
    "averaged_preds = (LogReg_pred + DTree_pred + SVC_pred)//3\n",
    "acc = accuracy_score(y_eval, averaged_preds)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d32100",
   "metadata": {},
   "source": [
    "## Bagging Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62ac47a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging with Logistic regression 0.7964634146341464\n",
      "Bagging with Decision trees 0.8126219512195121\n",
      "Bagging with Extra trees 0.7876219512195123\n",
      "Bagging with Random forest 0.8088719512195122\n"
     ]
    }
   ],
   "source": [
    "logreg_bagging_model = BaggingClassifier(\n",
    "    base_estimator=LogReg_clf, n_estimators=50, random_state=12)\n",
    "dtree_bagging_model = BaggingClassifier(\n",
    "    base_estimator=DTree_clf, n_estimators=50, random_state=12)\n",
    "extra_trees_model = ExtraTreesClassifier(n_estimators=100, random_state=12)\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=12)\n",
    "\n",
    "\n",
    "def bagging_ensemble(name, model):\n",
    "    k_folds = KFold(n_splits=20, random_state=12, shuffle=True)\n",
    "    # Array of scores of the estimator for each run of the cross validation.(nd-array)\n",
    "    scores = cross_val_score(model, x_train, y_train, cv=k_folds)\n",
    "    print(name, scores.mean())\n",
    "\n",
    "\n",
    "bagging_ensemble(\"Bagging with Logistic regression\", logreg_bagging_model)\n",
    "bagging_ensemble(\"Bagging with Decision trees\", dtree_bagging_model)\n",
    "bagging_ensemble(\"Bagging with Extra trees\", extra_trees_model)\n",
    "bagging_ensemble(\"Bagging with Random forest\", random_forest_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09094ea7",
   "metadata": {},
   "source": [
    "## Boosting Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a6d48bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost with Decision trees:\n",
      "Result of 20 estimators: 0.8052134146341464\n",
      "Result of 40 estimators: 0.8176524390243903\n",
      "Result of 60 estimators: 0.8176829268292682\n",
      "Result of 80 estimators: 0.8114024390243901\n",
      "Result of 100 estimators: 0.8114024390243904\n"
     ]
    }
   ],
   "source": [
    "print(\"AdaBoost with Decision trees:\")\n",
    "k_folds = KFold(n_splits=20, random_state=12, shuffle=True)\n",
    "num_estimators = [20, 40, 60, 80, 100]\n",
    "\n",
    "for num_estimator in num_estimators:\n",
    "    # default is DecisionTreeClassifier\n",
    "    ada_boost_clf = AdaBoostClassifier(\n",
    "        n_estimators=num_estimator, random_state=12)\n",
    "    scores = cross_val_score(ada_boost_clf, x_train, y_train, cv=k_folds)\n",
    "    print(f\"Result of {num_estimator} estimators: {scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290c946",
   "metadata": {},
   "source": [
    "## voting\\Stacking Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "161a4960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting classifier: 0.8301524390243902\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators=[(\"SVC\", SVC_clf), (\"DTs\", DTree_clf), (\"Logreg\", LogReg_clf)], voting=\"soft\")\n",
    "\n",
    "scores = cross_val_score(voting_clf, x_train, y_train, cv=k_folds)\n",
    "\n",
    "print(f\"Voting classifier: {scores.mean()}\")\n",
    "\n",
    "# About the accuracy of the model\n",
    "# pred_eval = voting_clf.predict(x_eval)\n",
    "# print(\"Voting classifier:\")\n",
    "# print(f\"Accurracy: {accuracy_score(y_eval, pred_eval)}\")\n",
    "# print(f\"Log loss : {log_loss(y_eval, pred_eval)}\")\n",
    "# print(f\"F1 score : {f1_score(y_eval, pred_eval)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe58dbc",
   "metadata": {},
   "source": [
    "## AdaBoost with Voting Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aedc530e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost classifier with SVC, DTs and LR\n",
      "Result of 1 estimators: 0.8014024390243903\n",
      "Result of 5 estimators: 0.8126219512195121\n",
      "Result of 10 estimators: 0.8001219512195122\n",
      "Result of 15 estimators: 0.8013719512195122\n"
     ]
    }
   ],
   "source": [
    "print(\"AdaBoost classifier with SVC, DTs and LR\")\n",
    "num_estimators = [1, 5, 10, 15]\n",
    "\n",
    "for num_estimator in num_estimators:\n",
    "    ada_boost_clf = AdaBoostClassifier(\n",
    "        base_estimator=voting_clf, n_estimators=num_estimator, random_state=12)\n",
    "    ada_boost_clf.fit(x_train, y_train)\n",
    "    scores = cross_val_score(ada_boost_clf, x_train, y_train, cv=k_folds)\n",
    "    print(f\"Result of {num_estimator} estimators: {scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc5ca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_file_for_submission(filename, passenger_id_list, prediction):\n",
    "    \"\"\"\n",
    "    passenger_id_list: 1D-array for passenfer ID\n",
    "    prediction: 1D-array for prediction\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(np.c_[passenger_id_list, prediction],\n",
    "                      columns=[\"PassengerId\", \"Survived\"])\n",
    "    df.to_csv(path_or_buf=filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f52653",
   "metadata": {},
   "source": [
    "![](data/screenshot.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
